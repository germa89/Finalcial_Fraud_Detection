{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Title:** Fraud Detection on Credit Card records using Random Forest.   \n",
    "**Author:** German Martinez-Ayuso   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A not very formal, neither comprehensive guide to the application of the gradient boosted decision trees to the detection of fraud in finance.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is decision tool whose structure resemble a tree. Each of the nodes of this structure represents a \"test\" (or evaluation of an activation function). Using the results of these test, the classification rules are obtained which goes from the root (input) to the leaves (outputs). \n",
    "\n",
    "One example of decision tree is the Galton machine:\n",
    "\n",
    "|![Galton machine](galton_machine.gif)| \n",
    "|:------:|\n",
    "|   [Galton machine](https://en.wikipedia.org/wiki/File:Galton_box.webm) |\n",
    "|From [Wikipedia-Beam machine](https://en.wikipedia.org/wiki/Bean_machine)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the pegs is a test where the ball will be driven to one of the sides. Since all the pegs and balls are equal, the probability of the ball going to one side or another is the same. Since it is the same, the balls will tend to be closer to the center because the probability of them going to the extremes is lower. This machine generate a distribution of the balls which corresponds to the \"**normal distribution**\". \n",
    "\n",
    "\n",
    "\n",
    "| ![Galton board](galton-board-with-normal-distribution-and-gaussian-bell-curve.jpg) | \n",
    "|:------:|\n",
    "|   Galton representation  |\n",
    "| Copyright : Peter Hermes Furian. [123RF.com](https://www.123rf.com/photo_79407706_stock-vector-the-mathematics-of-the-galton-board-with-normal-distribution-and-gaussian-bell-curve-also-quincunx-b.html) |\n",
    "\n",
    "The decision trees present several advantages. The main one is that they are easy to understand and interpret hence these models are widely applied in order to obtain a good understanding of the underlying model.\n",
    "\n",
    "\n",
    "## Decision Tree Structure\n",
    "The decision trees are composed of:\n",
    "- Nodes. Where the branches growth from. Here is where the tests are applied.\n",
    "- Branches. They represents the links between nodes.\n",
    "- Leaves. They are the final output at the end of the branches. They might not be applied any tests. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Decision Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost is the solution for the optimization of assemblies of decision trees. It is based on the concept that decision trees can be considered as a function which can be optimized using gradient techniques as any common function. \n",
    "This optimization would be \"measured\" using a loss function. \n",
    "This function is a measure of the error in the predicted values compared with the real values in the training set.\n",
    "\n",
    "The algorithm follows for the next inputs:\n",
    "- Training set: $\\{(x_i, y_i)\\}^n_{i=1}$ \n",
    "- Differentiable loss function: $L(y,F(x))$ \n",
    "- Maximum number of iterations: $M$\n",
    "\n",
    "the next steps:\n",
    "\n",
    "1. Initialize the model with random values.\n",
    "\n",
    "$$F_0(x) = \\arg \\min \\sum^n_{i=1} L(y_i, \\gamma)$$\n",
    "\n",
    "2. For m=1 to M:  \n",
    "\n",
    "    2.1. Compute so-called pseudo-residuals:\n",
    "    $$ r_{im} = - \\left[ \\dfrac{\\partial L(y_i,F(x_i))}{\\partial F(x_i)} \\right]_{F(x) = F_{m-1}(x)} \\hspace{3ex} \\text{for i = 1,  ..., n}$$   \n",
    "    \n",
    "    2.2. Fit a base learner (or weak learner e.g. tree) $h_m(x)$ to pseudo-residuals, i.e. train it using the training set.     \n",
    "    \n",
    "    2.3. Compute multiplier $\\gamma_m$ by solving the one-dimensional optimization problem:\n",
    "    $$ \\gamma_m = \\arg \\min \\sum^n_{i=1} L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))$$   \n",
    "    \n",
    "    2.4. Update the model:\n",
    "    $$ F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)$$\n",
    "\n",
    "3. Output $F_M(x)$.\n",
    "\n",
    "This is as it is explained in Wikipedia. However in a more simplistic way, this procedure could be explained as:\n",
    "\n",
    "1. Initialize the model with random values ($x_0$).\n",
    "2. Start iterations until maximum number of iterations is reached. \n",
    "\n",
    "    2.1. Calculate the increments in the loss function for the values ($x_m$).  \n",
    "    2.2. Train the decision trees ($h_m(x)$) to fit the increments.  \n",
    "    2.3. Obtain $\\gamma$ from the optimization problem given the trained trees.  \n",
    "    2.4. Update the model ($F_m(x)$).   \n",
    "    \n",
    "It should be noticed that the decision trees are trained on the residual, hence their results $h_m(x)$ are added to the solution in the previous iteration $F_{m-1}(x)$ multiplied by $\\gamma_m$. \n",
    "\n",
    "\n",
    "## Avoiding overfitting.\n",
    "\n",
    "Overfitting is the what happens when the model tends to replicate very accurate the training set but it does not perform well on new data. \n",
    "This is considered as a reduction in the generality of the model and it is addressed by using regularization techniques:\n",
    "\n",
    "### Shrinkage\n",
    "This consists in adding another multiplier to the $\\gamma_m$ coefficient:  \n",
    "$$ F_m(x) = F_{m-1}(x) + \\nu \\gamma_m h_m(x)  \\hspace{3ex} 0 < \\nu \\leq 1$$   \n",
    "where $\\nu$ is called \"learning rate\". \n",
    "\n",
    "This improves substantially the performance the model. Empirically it is demonstrated that values of $\\nu$ lower than 0.1 are the best. \n",
    "\n",
    "\n",
    "### Stochastic gradient boosting\n",
    "\n",
    "Stochastic gradient boosting take the idea from the bootstrap aggregation (\"bagging\") method. \n",
    "The idea consists on train each of the trees ($h_m(x)$) in a subset of the training set randomly generated without replacement.\n",
    "It has been observed that this improve substantially the model accuracy. \n",
    "The subsample size is represented as a fraction $f$ of the total set. If $f=1$ this improvement is not applied. Practically it has been observed that for moderate or small size dataset, the optimal $f$ values are between 0.5 and 0.8.\n",
    "Since the subsets are smaller, there is generally an improvement in the training speed. \n",
    "\n",
    "This approach might leave out some observations when building the learners (trees). These observation can be used as a validation inside the scheme hence it is not needed a validation set. The error between these leave-out observations and the model predictions is called \"out-of-bag error. \n",
    "\n",
    "\n",
    "### Number of observation in leaves.\n",
    "Ensuring that there is, at least, a minimum number of observations in each of the leaves of the trees is another way to help to regularize the model. \n",
    "\n",
    "\n",
    "### Penalize tree complexity\n",
    "The complexity of the tree is measured as the number of leaves. Using a suitable pruning approach it might help to reduce the number of leaves by removing leaves or branches that do not contribute to reduce the loss by a threshold. \n",
    "\n",
    "\n",
    "--  \n",
    "Reference: [Wikipedia-Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of XGBoost to Finance Fraud detection\n",
    "\n",
    "## Dataset: Credit card transfers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File creditcard.csv does not exist: 'creditcard.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-812ca3880d8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"creditcard.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File creditcard.csv does not exist: 'creditcard.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables are not defined, then we are going to use a black-input approach. We won't do an exploratory analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    df.drop('Class',axis=1),\n",
    "                                    df['Class'],\n",
    "                                    test_size=0.3,\n",
    "                                    train_size=0.7,\n",
    "                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score as measure_roc_auc\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_bin=16)\n",
    "\n",
    "# Space search for the parameters.\n",
    "prams={\n",
    "    'learning_rate':[0.01,0.03,0.05,0.1,0.15,0.2], #Step size shrinkage used to prevent overfitting. Range is [0,1]\n",
    "    'n_estimators':[100,200,500,1000,2000],        #Number of trees you want to build.\n",
    "    'max_depth':[3,5,10],                          #The maximum depth of a tree\n",
    "    'colsample_bytree':[0.1,0.3,0.5,1],            #Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    'subsample':[0.1,0.3,0.5,1]                    #Denotes the fraction of observations to be randomly samples for each tree.\n",
    "}\n",
    "\n",
    "prams={\n",
    "    'learning_rate':[0.05], #Step size shrinkage used to prevent overfitting. Range is [0,1]\n",
    "    'n_estimators':[100],        #Number of trees you want to build.\n",
    "    'max_depth':[3],                          #The maximum depth of a tree\n",
    "    'colsample_bytree':[0.1],            #Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    'subsample':[0.1]                    #Denotes the fraction of observations to be randomly samples for each tree.\n",
    "}\n",
    "\n",
    "\n",
    "#RandomizedSearchCV search for the optimal set of hyperparameters\n",
    "opt_xgb=RandomizedSearchCV(xgb,\n",
    "                              param_distributions=prams,\n",
    "                              verbose=2,\n",
    "                              n_iter=20,\n",
    "                              cv=10,\n",
    "                              scoring='roc_auc',\n",
    "                              n_jobs=2 # To speed up calculations\n",
    "                             );\n",
    "\n",
    "opt_xgb.fit(X_train, y_train) #Training\n",
    "\n",
    "pprint(opt_xgb.best_params_)\n",
    "print(\"\\nThe AUC score is {}\".format(opt_xgb.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "%store opt_xgb df \n",
    "# backup = pd.HDFStore('backup.h5')\n",
    "# backup['df'] = df\n",
    "# backup['opt_xgb'] = opt_xgb\n",
    "# backup.close()\n",
    "\n",
    "#load \n",
    "# %store -r opt_xgb df \n",
    "# backup = pd.HDFStore('backup.h5')\n",
    "# df = backup['df']\n",
    "# opt_xgb = backup['opt_xgb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = opt_xgb.predict_proba(X_test)\n",
    "accur_training = measure_roc_auc(y_test, predict_y[:, 1])\n",
    "\n",
    "print(\"The test AUC for training data is: {:0.6f}\".format(accur_training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC score is 0.962289 on the test samples. The AUC score is the area under the curve ROC which is the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "i=1\n",
    "\n",
    "y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, predict_y[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), predict_y.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "# Plot of a ROC curve for a specific class\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "model = opt_xgb.best_estimator_\n",
    " \n",
    "plot_importance(model, importance_type='gain', max_num_features=10); "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
